{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174fe984-d928-43bb-b9ff-565f6216a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cda0acf-e83e-4ea0-951b-0d77db8b9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Styles:\n",
    "    VIOLET = \"\\033[95m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    CYAN = \"\\033[96m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    YELLOW = \"\\033[93m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    END = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\"\n",
    "    \n",
    "\n",
    "def highlight(s, color=Styles.CYAN):\n",
    "    s = str(s)\n",
    "    split_s = s.split(Styles.END)\n",
    "    highlighted_split_s = [f\"{color}{Styles.BOLD}{s}\" for s in split_s]\n",
    "    highlight_s = Styles.END.join(highlighted_split_s) + Styles.END\n",
    "    return highlight_s\n",
    "\n",
    "\n",
    "def highlight_words(s, indexes, color=Styles.CYAN):\n",
    "    split_s = s.split(\" \")\n",
    "    for index in indexes:\n",
    "        split_s[index] = highlight(split_s[index], color=color)\n",
    "    return \" \".join(split_s)\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0646f396-34a6-495c-a40c-9dafb0d5f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_jpg_df = pd.read_csv('../mimic-cxr-2.0.0-metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3476fc-e2f6-478a-98b9-680491270207",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_cxr_jpg_df = mimic_cxr_jpg_df[['subject_id', 'study_id', 'StudyDate', 'StudyTime']]\n",
    "mimic_cxr_jpg_df = mimic_cxr_jpg_df.drop_duplicates()\n",
    "mimic_cxr_jpg_df = mimic_cxr_jpg_df.sort_values(['subject_id', 'StudyDate', 'StudyTime'], ascending=True)\n",
    "mimic_cxr_jpg = mimic_cxr_jpg_df.values.tolist()\n",
    "mimic_cxr_jpg = {\n",
    "    (str(int(v[0])), str(int(v[1]))): (v[2], v[3])\n",
    "    for v in mimic_cxr_jpg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c6895c-e060-4809-9f8c-88bc409cd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationType(Enum):\n",
    "    NER = 0\n",
    "    RELATIONS = 1\n",
    "    TASK_1 = 2\n",
    "    TASK_2 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365debb7-f975-4b2a-8a32-0afc9a87b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_PATH = '../../models/dygie-radgraph-base/predictions_chexpert_test.jsonl'\n",
    "PREDICTIONS_FILE = PREDICTIONS_PATH\n",
    "LABELS_PATH = '../../data/dygie_test_chexpert.json'\n",
    "LABELS_FILE = LABELS_PATH\n",
    "RADGRAPH_CLASSES = ['ANAT-DP', 'OBS-DP', 'OBS-DA', 'OBS-U', 'CHAN-IMP', 'CHAN-WOR', 'CHAN-AP', 'CHAN-DISA', 'CHAN-DISP', 'CHAN-NC']\n",
    "RADGRAPH_RELATION_CLASSES = ['modify', 'located_at', 'suggestive_of']\n",
    "CHANGE_CLASSES = ['CHAN-IMP', 'CHAN-WOR', 'CHAN-AP', 'CHAN-DISA', 'CHAN-DISP']\n",
    "TASK_1_CLASSES = ['NO CHANGE', 'CHANGE']\n",
    "TASK_2_CLASSES = CHANGE_CLASSES + ['CHAN-NC']\n",
    "\n",
    "with open(PREDICTIONS_FILE, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ffccb0-1d6e-4acb-9f7b-9629bfceefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_studies_dict = defaultdict(lambda: {'change_mention': False, 'studies': []})\n",
    "for data_sample in data:\n",
    "    _, patient_id, study_id = data_sample['doc_key'].split('/')\n",
    "    patient_id = patient_id[1:]\n",
    "    study_id = study_id[1:-4]\n",
    "    if patient_id == '':\n",
    "        continue\n",
    "    \n",
    "    labels = data_sample['ner'][0]\n",
    "    predictions = [[item[:3] for item in prediction] for prediction in data_sample['predicted_ner']][0]\n",
    "    \n",
    "    text = \" \".join(data_sample['sentences'][0])\n",
    "    study_data = {\n",
    "        \"text\": \" \".join(data_sample['sentences'][0]),\n",
    "        \"study_id\": study_id,\n",
    "        \"labels\": {},\n",
    "        \"predictions\": {},\n",
    "        \"change_mention\": False,\n",
    "        \"time\": mimic_cxr_jpg[(patient_id, study_id)],\n",
    "        \"highlight_idxs\": set()\n",
    "    }\n",
    "\n",
    "    change_mention = False\n",
    "    highlight_idxs = set()\n",
    "    radgraph_classes = TASK_2_CLASSES\n",
    "    for radgraph_class in radgraph_classes:\n",
    "        labels_of_class = set([(\" \".join(text.split()[label[0]:label[1] + 1]), label[0], label[1]) for label in labels if label[2] == radgraph_class])\n",
    "        predictions_of_class = set([(\" \".join(text.split()[prediction[0]:prediction[1] + 1]), prediction[0], prediction[1]) for prediction in predictions if prediction[2] == radgraph_class])\n",
    "        highlight_idxs.update(flatten([list(range(v[1], v[2] + 1)) for v in labels_of_class]))\n",
    "        highlight_idxs.update(flatten([list(range(v[1], v[2] + 1)) for v in predictions_of_class]))\n",
    "        if len(labels_of_class) > 0:\n",
    "            change_mention = True\n",
    "        if len(predictions_of_class) > 0:\n",
    "            change_mention = True\n",
    "        study_data['labels'][radgraph_class] = labels_of_class\n",
    "        study_data['predictions'][radgraph_class] = predictions_of_class\n",
    "        study_data['change_mention'] = change_mention\n",
    "    study_data['highlight_idxs'] = highlight_idxs\n",
    "        \n",
    "    patient_studies_dict[patient_id]['change_mention'] = patient_studies_dict[patient_id]['change_mention'] or change_mention\n",
    "    patient_studies_dict[patient_id]['studies'].append(study_data)\n",
    "\n",
    "patient_studies_dict = {\n",
    "    k: sorted(v['studies'], key=lambda s: s['time'])\n",
    "    for k, v in patient_studies_dict.items()\n",
    "    if v['change_mention'] and len(v['studies']) > 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e22e37-5099-4fc3-8056-a3e9d2b31f4a",
   "metadata": {},
   "source": [
    "## Patient timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec651469-134a-4680-811d-a6f1487f75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id, patient_studies in patient_studies_dict.items():\n",
    "    print(f\"###############################################################\")\n",
    "    print(f\"################ PATIENT {patient_id} #########################\")\n",
    "    print(f\"###############################################################\")\n",
    "    for study in patient_studies:\n",
    "        print()\n",
    "        print(f\"================== Study {study['study_id']} ==================\")\n",
    "        print(f\"Time: {study['time']}\")\n",
    "        print(highlight_words(study['text'], study['highlight_idxs']))\n",
    "        print(f\"Labels: {study['labels']}\")\n",
    "        print(f\"Predictions: {study['predictions']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216e7adc-c0ca-4868-b520-25645d924b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(relations):\n",
    "    metrics_dict = defaultdict(lambda: {'tps': 0, 'fps': 0, 'fns': 0, 'total_actual': 0, 'total_predicted': 0})\n",
    "    for data_sample in data:\n",
    "        labels = data_sample['relations' if relations else 'ner'][0]\n",
    "        predictions = [[item[:(5 if relations else 3)] for item in prediction] for prediction in data_sample['predicted_relations' if relations else 'predicted_ner']][0]\n",
    "\n",
    "        radgraph_classes = RADGRAPH_RELATION_CLASSES if relations else RADGRAPH_CLASSES\n",
    "        for radgraph_class in radgraph_classes:\n",
    "            labels_of_class = set([tuple(label) for label in labels if label[4 if relations else 2] == radgraph_class])\n",
    "            predictions_of_class = set([tuple(prediction) for prediction in predictions if prediction[4 if relations else 2] == radgraph_class])\n",
    "            metrics_dict[radgraph_class]['tps'] += len(labels_of_class & predictions_of_class)\n",
    "            metrics_dict[radgraph_class]['fps'] += len(predictions_of_class - labels_of_class)\n",
    "            metrics_dict[radgraph_class]['fns'] += len(labels_of_class - predictions_of_class)\n",
    "            metrics_dict[radgraph_class]['total_actual'] += len(labels_of_class)\n",
    "            metrics_dict[radgraph_class]['total_predicted'] += len(predictions_of_class)\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def compute_metrics_task_1():\n",
    "    metrics_dict = defaultdict(lambda: {'tps': 0, 'fps': 0, 'fns': 0, 'total_actual': 0, 'total_predicted': 0})\n",
    "    for data_sample in data:\n",
    "        labels = [item[2] for item in data_sample['ner'][0]]\n",
    "        predictions = [[item[2] for item in prediction] for prediction in data_sample['predicted_ner']][0]\n",
    "        \n",
    "        chan_nc_label = 0\n",
    "        chan_nc_prediction = 0\n",
    "        if 'CHAN-NC' in labels:\n",
    "            chan_nc_label = 1\n",
    "        if 'CHAN-NC' in predictions:\n",
    "            chan_nc_prediction = 1\n",
    "            \n",
    "        chan_label = 0\n",
    "        chan_prediction = 0\n",
    "        if any([change_class in labels for change_class in CHANGE_CLASSES]):\n",
    "            chan_label = 1\n",
    "        if any([change_class in predictions for change_class in CHANGE_CLASSES]):\n",
    "            chan_prediction = 1\n",
    "            \n",
    "        metrics_dict['NO CHANGE']['tps'] += chan_nc_label & chan_nc_prediction\n",
    "        metrics_dict['NO CHANGE']['fps'] += max(0, chan_nc_prediction - chan_nc_label)\n",
    "        metrics_dict['NO CHANGE']['fns'] += max(0, chan_nc_label - chan_nc_prediction)\n",
    "        metrics_dict['NO CHANGE']['total_actual'] += chan_nc_label\n",
    "        metrics_dict['NO CHANGE']['total_predicted'] += chan_nc_prediction\n",
    "        metrics_dict['CHANGE']['tps'] += chan_label & chan_prediction\n",
    "        metrics_dict['CHANGE']['fps'] += max(0, chan_prediction - chan_label)\n",
    "        metrics_dict['CHANGE']['fns'] += max(0, chan_label - chan_prediction)\n",
    "        metrics_dict['CHANGE']['total_actual'] += chan_label\n",
    "        metrics_dict['CHANGE']['total_predicted'] += chan_prediction\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def compute_metrics_task_2():\n",
    "    metrics_dict = defaultdict(lambda: {'tps': 0, 'fps': 0, 'fns': 0, 'total_actual': 0, 'total_predicted': 0})\n",
    "    for data_sample in data:\n",
    "        labels = data_sample['ner'][0]\n",
    "        predictions = [[item[:3] for item in prediction] for prediction in data_sample['predicted_ner']][0]\n",
    "            \n",
    "        radgraph_classes = TASK_2_CLASSES\n",
    "        for radgraph_class in radgraph_classes:\n",
    "            labels_of_class = set([tuple(label) for label in labels if label[2] == radgraph_class])\n",
    "            label_of_class = 1 if len(labels_of_class) > 0 else 0\n",
    "            predictions_of_class = set([tuple(prediction) for prediction in predictions if prediction[2] == radgraph_class])\n",
    "            prediction_of_class = 1 if len(predictions_of_class) > 0 else 0\n",
    "            metrics_dict[radgraph_class]['tps'] += label_of_class & prediction_of_class\n",
    "            metrics_dict[radgraph_class]['fps'] +=  max(0, prediction_of_class - label_of_class)\n",
    "            metrics_dict[radgraph_class]['fns'] += max(0, label_of_class - prediction_of_class)\n",
    "            metrics_dict[radgraph_class]['total_actual'] += label_of_class\n",
    "            metrics_dict[radgraph_class]['total_predicted'] += prediction_of_class\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc43f0f-608a-40d3-bfc8-398a063f337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics_dict, evaluation_type):\n",
    "    if evaluation_type is EvaluationType.NER: \n",
    "        radgraph_classes = RADGRAPH_CLASSES\n",
    "    elif evaluation_type is EvaluationType.RELATIONS:\n",
    "        radgraph_classes = RADGRAPH_RELATION_CLASSES\n",
    "    elif evaluation_type is EvaluationType.TASK_1:\n",
    "        radgraph_classes = TASK_1_CLASSES\n",
    "    elif evaluation_type is EvaluationType.TASK_2:\n",
    "        radgraph_classes = TASK_2_CLASSES\n",
    "    total_tps = 0\n",
    "    total_fps = 0\n",
    "    total_fns = 0\n",
    "    macro_precision = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for radgraph_class in radgraph_classes:\n",
    "        tps = np.float64(metrics_dict[radgraph_class]['tps'])\n",
    "        total_tps += tps\n",
    "        fps = np.float64(metrics_dict[radgraph_class]['fps'])\n",
    "        total_fps += fps\n",
    "        fns = np.float64(metrics_dict[radgraph_class]['fns'])\n",
    "        total_fns += fns\n",
    "        total_actual = metrics_dict[radgraph_class]['total_actual']\n",
    "        total_predicted = metrics_dict[radgraph_class]['total_predicted']\n",
    "        precision = tps / (tps + fps)\n",
    "        macro_precision += np.nan_to_num(precision, nan=0)\n",
    "        recall = tps / (tps + fns)\n",
    "        macro_recall += np.nan_to_num(recall, nan=0)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        macro_f1 += np.nan_to_num(f1, nan=0)\n",
    "        print(f\"* Class {radgraph_class}\")\n",
    "        print(f\"  - Precision: {precision}\")\n",
    "        print(f\"  - Recall: {recall}\")\n",
    "        print(f\"  - F1: {f1}\")\n",
    "        print(f\"  - Total actual: {total_actual}\")\n",
    "        print(f\"  - Total predicted: {total_predicted}\")\n",
    "    micro_precision = total_tps / (total_tps + total_fps)\n",
    "    micro_recall = total_tps / (total_tps + total_fns)\n",
    "    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "    macro_precision /= len(radgraph_classes)\n",
    "    macro_recall /= len(radgraph_classes)\n",
    "    macro_f1 /= len(radgraph_classes)\n",
    "    print()\n",
    "    print(f\"* Micro precision: {micro_precision}\")\n",
    "    print(f\"* Micro recall: {micro_recall}\")\n",
    "    print(f\"* Micro F1: {micro_f1}\")\n",
    "    print(f\"* Macro precision: {macro_precision}\")\n",
    "    print(f\"* Macro recall: {macro_recall}\")\n",
    "    print(f\"* Macro F1: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47e574-9dfd-4e16-914e-a96d7af53954",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0d93cc-a37a-4d95-bb81-b40424334c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Class ANAT-DP\n",
      "  - Precision: 0.9461172741679873\n",
      "  - Recall: 0.9313572542901716\n",
      "  - F1: 0.9386792452830188\n",
      "  - Total actual: 641\n",
      "  - Total predicted: 631\n",
      "* Class OBS-DP\n",
      "  - Precision: 0.7635467980295566\n",
      "  - Recall: 0.8469945355191257\n",
      "  - F1: 0.8031088082901554\n",
      "  - Total actual: 549\n",
      "  - Total predicted: 609\n",
      "* Class OBS-DA\n",
      "  - Precision: 0.9047619047619048\n",
      "  - Recall: 0.9047619047619048\n",
      "  - F1: 0.9047619047619048\n",
      "  - Total actual: 168\n",
      "  - Total predicted: 168\n",
      "* Class OBS-U\n",
      "  - Precision: 0.5441176470588235\n",
      "  - Recall: 0.7872340425531915\n",
      "  - F1: 0.6434782608695653\n",
      "  - Total actual: 47\n",
      "  - Total predicted: 68\n",
      "* Class CHAN-IMP\n",
      "  - Precision: 1.0\n",
      "  - Recall: 1.0\n",
      "  - F1: 1.0\n",
      "  - Total actual: 4\n",
      "  - Total predicted: 4\n",
      "* Class CHAN-WOR\n",
      "  - Precision: 0.7391304347826086\n",
      "  - Recall: 0.9444444444444444\n",
      "  - F1: 0.8292682926829269\n",
      "  - Total actual: 18\n",
      "  - Total predicted: 23\n",
      "* Class CHAN-AP\n",
      "  - Precision: 0.25\n",
      "  - Recall: 1.0\n",
      "  - F1: 0.4\n",
      "  - Total actual: 1\n",
      "  - Total predicted: 4\n",
      "* Class CHAN-DISA\n",
      "  - Precision: 0.5\n",
      "  - Recall: 0.6666666666666666\n",
      "  - F1: 0.5714285714285715\n",
      "  - Total actual: 3\n",
      "  - Total predicted: 4\n",
      "* Class CHAN-DISP\n",
      "  - Precision: 0.0\n",
      "  - Recall: 0.0\n",
      "  - F1: nan\n",
      "  - Total actual: 4\n",
      "  - Total predicted: 3\n",
      "* Class CHAN-NC\n",
      "  - Precision: 0.847457627118644\n",
      "  - Recall: 0.7936507936507936\n",
      "  - F1: 0.819672131147541\n",
      "  - Total actual: 63\n",
      "  - Total predicted: 59\n",
      "\n",
      "* Micro precision: 0.8423394787031151\n",
      "* Micro recall: 0.8845126835781041\n",
      "* Micro F1: 0.8629111038749593\n",
      "* Macro precision: 0.6495131685919524\n",
      "* Macro recall: 0.7875109641886299\n",
      "* Macro F1: 0.6910397214463684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data1/hms/dbmi/rajpurkar/lab/home/ad461/project/radgraph/radgraph-env/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = compute_metrics(False)\n",
    "print_metrics(metrics_dict, EvaluationType.NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa91992a-def2-4308-b79c-c7d6b6aaef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Class modify\n",
      "  - Precision: 0.7008196721311475\n",
      "  - Recall: 0.6885906040268457\n",
      "  - F1: 0.6946513202437373\n",
      "  - Total actual: 745\n",
      "  - Total predicted: 732\n",
      "* Class located_at\n",
      "  - Precision: 0.803030303030303\n",
      "  - Recall: 0.7636887608069164\n",
      "  - F1: 0.7828655834564253\n",
      "  - Total actual: 347\n",
      "  - Total predicted: 330\n",
      "* Class suggestive_of\n",
      "  - Precision: 0.6862745098039216\n",
      "  - Recall: 0.5303030303030303\n",
      "  - F1: 0.5982905982905983\n",
      "  - Total actual: 66\n",
      "  - Total predicted: 51\n",
      "\n",
      "* Micro precision: 0.7304582210242587\n",
      "* Micro recall: 0.7020725388601037\n",
      "* Micro F1: 0.7159841479524439\n",
      "* Macro precision: 0.7300414949884573\n",
      "* Macro recall: 0.6608607983789309\n",
      "* Macro F1: 0.6919358339969204\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = compute_metrics(True)\n",
    "print_metrics(metrics_dict, EvaluationType.RELATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14a0d0-4258-4530-b3e0-4c53e32bc795",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "483bbef7-c8c7-4e4c-affb-b2ea20fc0867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Class NO CHANGE\n",
      "  - Precision: 0.88\n",
      "  - Recall: 1.0\n",
      "  - F1: 0.9361702127659575\n",
      "  - Total actual: 22\n",
      "  - Total predicted: 25\n",
      "* Class CHANGE\n",
      "  - Precision: 0.8\n",
      "  - Recall: 0.8888888888888888\n",
      "  - F1: 0.8421052631578948\n",
      "  - Total actual: 18\n",
      "  - Total predicted: 20\n",
      "\n",
      "* Micro precision: 0.8444444444444444\n",
      "* Micro recall: 0.95\n",
      "* Micro F1: 0.8941176470588236\n",
      "* Macro precision: 0.8400000000000001\n",
      "* Macro recall: 0.9444444444444444\n",
      "* Macro F1: 0.8891377379619261\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = compute_metrics_task_1()\n",
    "print_metrics(metrics_dict, EvaluationType.TASK_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372fa2b-af2a-4efb-82cd-f46b973a4cbc",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bb2558-7c07-4775-b15e-3dbb4c5541d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Class CHAN-IMP\n",
      "  - Precision: 1.0\n",
      "  - Recall: 1.0\n",
      "  - F1: 1.0\n",
      "  - Total actual: 3\n",
      "  - Total predicted: 3\n",
      "* Class CHAN-WOR\n",
      "  - Precision: 0.6666666666666666\n",
      "  - Recall: 1.0\n",
      "  - F1: 0.8\n",
      "  - Total actual: 10\n",
      "  - Total predicted: 15\n",
      "* Class CHAN-AP\n",
      "  - Precision: 0.3333333333333333\n",
      "  - Recall: 1.0\n",
      "  - F1: 0.5\n",
      "  - Total actual: 1\n",
      "  - Total predicted: 3\n",
      "* Class CHAN-DISA\n",
      "  - Precision: 0.5\n",
      "  - Recall: 0.6666666666666666\n",
      "  - F1: 0.5714285714285715\n",
      "  - Total actual: 3\n",
      "  - Total predicted: 4\n",
      "* Class CHAN-DISP\n",
      "  - Precision: 0.6666666666666666\n",
      "  - Recall: 0.6666666666666666\n",
      "  - F1: 0.6666666666666666\n",
      "  - Total actual: 3\n",
      "  - Total predicted: 3\n",
      "* Class CHAN-NC\n",
      "  - Precision: 0.88\n",
      "  - Recall: 1.0\n",
      "  - F1: 0.9361702127659575\n",
      "  - Total actual: 22\n",
      "  - Total predicted: 25\n",
      "\n",
      "* Micro precision: 0.7547169811320755\n",
      "* Micro recall: 0.9523809523809523\n",
      "* Micro F1: 0.8421052631578947\n",
      "* Macro precision: 0.6744444444444445\n",
      "* Macro recall: 0.8888888888888888\n",
      "* Macro F1: 0.7457109084768659\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = compute_metrics_task_2()\n",
    "print_metrics(metrics_dict, EvaluationType.TASK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2967b1-35e5-4029-b8d0-76ebb8fa36c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8d22f-3612-4755-8818-34f356d8439a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
